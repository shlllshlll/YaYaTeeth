{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增强数据集生成程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理\n",
    "\n",
    "### 定义读写函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_summary_file(path):\n",
    "    summary_list = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line[-1] == '\\n':\n",
    "                line = line[:-1]\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            summary_list.append(line)\n",
    "        \n",
    "    return summary_list\n",
    "\n",
    "def write_summary_file(path, lines):\n",
    "    with open(path, 'w') as f:\n",
    "        for line in lines[:-1]:\n",
    "            f.write(line+'\\n')\n",
    "        f.write(lines[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些Path定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('/home/shlll/Dataset/Teeth/')\n",
    "\n",
    "dataset_base_path = base_path / 'aug1'\n",
    "summary_path = dataset_base_path / \"ImageSets/Segmentation\"\n",
    "train_summary_path = summary_path / 'train.txt'\n",
    "trainval_summary_path = summary_path / 'trainval.txt'\n",
    "val_summary_path = summary_path / 'val.txt'\n",
    "src_image_path = dataset_base_path / 'JPEGImages_700'\n",
    "gt_image_path = dataset_base_path / 'newSegmentationClass_700'\n",
    "\n",
    "aug_src_img_path = src_image_path / 'output'\n",
    "\n",
    "teeth_dataset_base_path = base_path / 'aug2'\n",
    "teeth_summary_path = teeth_dataset_base_path / \"ImageSets/Segmentation\"\n",
    "teeth_train_summary_path = teeth_summary_path / 'train.txt'\n",
    "teeth_trainval_summary_path = teeth_summary_path / 'trainval.txt'\n",
    "teeth_val_summary_path = teeth_summary_path / 'val.txt'\n",
    "teeth_src_image_path = teeth_dataset_base_path / 'JPEGImages'\n",
    "teeth_gt_image_path = teeth_dataset_base_path / 'SegmentationClass'\n",
    "\n",
    "teeth_aug_src_img_path = teeth_dataset_base_path / 'outputbright'\n",
    "teeth_aug_src_img_path_pro = teeth_dataset_base_path / 'outputbright_pro'\n",
    "\n",
    "teeth_aug_src_img_path_ori = teeth_dataset_base_path / 'outputori'\n",
    "teeth_aug_src_img_path_ori_pro = teeth_dataset_base_path / 'outputori_pro'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理已有数据集\n",
    "\n",
    "### 处理full-VOC20200422数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename数据集\n",
    "aug_src_image_list = map(lambda x: x.name, aug_src_img_path.glob('*.jpg'))\n",
    "\n",
    "last_name = None\n",
    "count = 1\n",
    "for path in sorted(aug_src_image_list):\n",
    "    part = path.split('_')[4]\n",
    "    \n",
    "    if part == last_name:\n",
    "        count += 1\n",
    "    else:\n",
    "        last_name = part\n",
    "        count = 1\n",
    "    \n",
    "    dst_name = '_'.join(path.split('_')[3:5])[:-4] + '_' + str(count) + '.jpg'\n",
    "    ori_path = aug_src_img_path / path\n",
    "    dst_path = aug_src_img_path / dst_name\n",
    "    ori_path.rename(dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理teeth数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename数据集\n",
    "if (teeth_aug_src_img_path_pro.exists()):\n",
    "    shutil.rmtree(teeth_aug_src_img_path_pro)\n",
    "teeth_aug_src_img_path_pro.mkdir()\n",
    "    \n",
    "teeth_aug_src_image_list = map(lambda x: x.name, teeth_aug_src_img_path.glob('*.jpg'))\n",
    "\n",
    "last_name = None\n",
    "count = 1\n",
    "for path in sorted(teeth_aug_src_image_list):\n",
    "    if not path.startswith('JPEGImages_original'):\n",
    "        continue\n",
    "    \n",
    "    dst_name = '_'.join(path.split('_')[2:4])[:-4]\n",
    "    if dst_name == last_name:\n",
    "        count += 1\n",
    "    else:\n",
    "        last_name = dst_name\n",
    "        count = 1\n",
    "    dst_name = dst_name + '_' + str(count) + '.jpg'        \n",
    "    \n",
    "    ori_path = teeth_aug_src_img_path / path\n",
    "    dst_path = teeth_aug_src_img_path_pro / dst_name\n",
    "    os.link(ori_path, dst_path)\n",
    "\n",
    "# Resize Image\n",
    "for path in teeth_aug_src_img_path_pro.glob('*.jpg'):\n",
    "    img = Image.open(path).resize((700, 700))\n",
    "    img.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename数据集\n",
    "if (teeth_aug_src_img_path_ori_pro.exists()):\n",
    "    shutil.rmtree(teeth_aug_src_img_path_ori_pro)\n",
    "teeth_aug_src_img_path_ori_pro.mkdir()\n",
    "    \n",
    "teeth_aug_src_image_list = map(lambda x: x.name, teeth_aug_src_img_path_ori.glob('*.jpg'))\n",
    "\n",
    "last_name = None\n",
    "count = 1\n",
    "for path in sorted(teeth_aug_src_image_list):\n",
    "    if not path.startswith('JPEGImages_original'):\n",
    "        continue\n",
    "    \n",
    "    dst_name = '_'.join(path.split('_')[2:4])[:-4]\n",
    "    if dst_name == last_name:\n",
    "        count += 1\n",
    "    else:\n",
    "        last_name = dst_name\n",
    "        count = 1\n",
    "    dst_name = dst_name + '_' + str(count) + '.jpg'        \n",
    "    \n",
    "    ori_path = teeth_aug_src_img_path_ori / path\n",
    "    dst_path = teeth_aug_src_img_path_ori_pro / dst_name\n",
    "    os.link(ori_path, dst_path)\n",
    "\n",
    "# Resize Image\n",
    "for path in teeth_aug_src_img_path_ori_pro.glob('*.jpg'):\n",
    "    img = Image.open(path).resize((700, 700))\n",
    "    img.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并成为新的数据集\n",
    "\n",
    "### 生成bright数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 路径配置\n",
    "dst_dataset_base_path = base_path / 'aug_bright'\n",
    "dst_summary_path = dst_dataset_base_path / \"ImageSets/Segmentation\"\n",
    "dst_train_summary_path = dst_summary_path / 'train.txt'\n",
    "dst_trainval_summary_path = dst_summary_path / 'trainval.txt'\n",
    "dst_val_summary_path = dst_summary_path / 'val.txt'\n",
    "dst_src_image_path = dst_dataset_base_path / 'JPEGImages'\n",
    "dst_gt_image_path = dst_dataset_base_path / 'SegmentationClass'\n",
    "\n",
    "# 删除已有的新数据集\n",
    "if (dst_dataset_base_path.exists()):\n",
    "    shutil.rmtree(dst_dataset_base_path)\n",
    "\n",
    "# 一些新数据集路径创建\n",
    "dst_dataset_base_path.mkdir(exist_ok=True)\n",
    "dst_summary_path.mkdir(parents=True, exist_ok=True)\n",
    "dst_src_image_path.mkdir(exist_ok=True)\n",
    "dst_gt_image_path.mkdir(exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 下面处理VOC数据集至新的数据集\n",
    "src_image_list = src_image_path.glob('*.jpg')\n",
    "gt_image_list = gt_image_path.glob('*.png')\n",
    "aug_src_image_list = aug_src_img_path.glob('*.jpg')\n",
    "\n",
    "# 移动原始数据集\n",
    "for path in src_image_list:\n",
    "    dst_path = dst_src_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "\n",
    "# 移动原始GT数据集\n",
    "for path in gt_image_list:\n",
    "    dst_path = dst_gt_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "\n",
    "# 移动aug数据集并生成对应的GT数据\n",
    "for path in aug_src_image_list:\n",
    "    gt_path = dst_gt_image_path / ('_'.join(path.stem.split('_')[:-1]) + '.png')\n",
    "    if not gt_path.exists():\n",
    "        continue\n",
    "    gt_dst_path = dst_gt_image_path / (path.stem + '.png')\n",
    "    dst_path = dst_src_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "    shutil.copy(gt_path, gt_dst_path)\n",
    "\n",
    "# ==========================================\n",
    "# 下面处理VOC数据集至新的数据集\n",
    "src_image_list = teeth_src_image_path.glob('*.jpg')\n",
    "gt_image_list = teeth_gt_image_path.glob('*.png')\n",
    "aug_src_image_list = teeth_aug_src_img_path_pro.glob('*.jpg')\n",
    "\n",
    "# 移动原始数据集\n",
    "for path in src_image_list:\n",
    "    dst_path = dst_src_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "\n",
    "# 移动原始GT数据集\n",
    "for path in gt_image_list:\n",
    "    dst_path = dst_gt_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "\n",
    "# 移动aug数据集并生成对应的GT数据\n",
    "for path in aug_src_image_list:\n",
    "    gt_path = dst_gt_image_path / ('_'.join(path.stem.split('_')[:-1]) + '.png')\n",
    "    if not gt_path.exists():\n",
    "        continue\n",
    "    gt_dst_path = dst_gt_image_path / (path.stem + '.png')\n",
    "    dst_path = dst_src_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "    shutil.copy(gt_path, gt_dst_path)\n",
    "\n",
    "# ==========================================\n",
    "# 下面处理两个数据集的Summary文件\n",
    "\n",
    "dst_train_list = []\n",
    "dst_trainval_list = []\n",
    "dst_val_list = []\n",
    "\n",
    "# 对应的summary文件\n",
    "train_list = read_summary_file(train_summary_path)\n",
    "trainval_list = read_summary_file(trainval_summary_path)\n",
    "val_list = read_summary_file(val_summary_path)\n",
    "teeth_train_list = read_summary_file(teeth_train_summary_path)\n",
    "teeth_trainval_list = read_summary_file(teeth_trainval_summary_path)\n",
    "teeth_val_list = read_summary_file(teeth_val_summary_path)\n",
    "train_list.extend(teeth_train_list)\n",
    "trainval_list.extend(teeth_trainval_list)\n",
    "val_list.extend(teeth_val_list)\n",
    "\n",
    "for name in train_list:\n",
    "    files = list(map(lambda x: x.stem, dst_src_image_path.glob(name + '*')))\n",
    "    dst_train_list.extend(files)\n",
    "    \n",
    "for name in trainval_list:\n",
    "    files = list(map(lambda x: x.stem, dst_src_image_path.glob(name + '*')))\n",
    "    dst_trainval_list.extend(files)\n",
    "    \n",
    "for name in val_list:\n",
    "    files = list(map(lambda x: x.stem, dst_src_image_path.glob(name + '*')))\n",
    "    dst_val_list.extend(files)\n",
    "    \n",
    "write_summary_file(dst_train_summary_path, dst_train_list)\n",
    "write_summary_file(dst_trainval_summary_path, dst_trainval_list)\n",
    "write_summary_file(dst_val_summary_path, dst_val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成ori数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 路径配置\n",
    "dst_dataset_base_path = base_path / 'aug_ori'\n",
    "dst_summary_path = dst_dataset_base_path / \"ImageSets/Segmentation\"\n",
    "dst_train_summary_path = dst_summary_path / 'train.txt'\n",
    "dst_trainval_summary_path = dst_summary_path / 'trainval.txt'\n",
    "dst_val_summary_path = dst_summary_path / 'val.txt'\n",
    "dst_src_image_path = dst_dataset_base_path / 'JPEGImages'\n",
    "dst_gt_image_path = dst_dataset_base_path / 'SegmentationClass'\n",
    "\n",
    "# 删除已有的新数据集\n",
    "if (dst_dataset_base_path.exists()):\n",
    "    shutil.rmtree(dst_dataset_base_path)\n",
    "\n",
    "# 一些新数据集路径创建\n",
    "dst_dataset_base_path.mkdir(exist_ok=True)\n",
    "dst_summary_path.mkdir(parents=True, exist_ok=True)\n",
    "dst_src_image_path.mkdir(exist_ok=True)\n",
    "dst_gt_image_path.mkdir(exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 下面处理VOC数据集至新的数据集\n",
    "src_image_list = src_image_path.glob('*.jpg')\n",
    "gt_image_list = gt_image_path.glob('*.png')\n",
    "aug_src_image_list = aug_src_img_path.glob('*.jpg')\n",
    "\n",
    "# 移动原始数据集\n",
    "for path in src_image_list:\n",
    "    dst_path = dst_src_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "\n",
    "# 移动原始GT数据集\n",
    "for path in gt_image_list:\n",
    "    dst_path = dst_gt_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "\n",
    "# 移动aug数据集并生成对应的GT数据\n",
    "for path in aug_src_image_list:\n",
    "    gt_path = dst_gt_image_path / ('_'.join(path.stem.split('_')[:-1]) + '.png')\n",
    "    if not gt_path.exists():\n",
    "        continue\n",
    "    gt_dst_path = dst_gt_image_path / (path.stem + '.png')\n",
    "    dst_path = dst_src_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "    shutil.copy(gt_path, gt_dst_path)\n",
    "\n",
    "# ==========================================\n",
    "# 下面处理teeth数据集至新的数据集\n",
    "src_image_list = teeth_src_image_path.glob('*.jpg')\n",
    "gt_image_list = teeth_gt_image_path.glob('*.png')\n",
    "aug_src_image_list = teeth_aug_src_img_path_ori_pro.glob('*.jpg')\n",
    "\n",
    "# 移动原始数据集\n",
    "for path in src_image_list:\n",
    "    dst_path = dst_src_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "\n",
    "# 移动原始GT数据集\n",
    "for path in gt_image_list:\n",
    "    dst_path = dst_gt_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "\n",
    "# 移动aug数据集并生成对应的GT数据\n",
    "for path in aug_src_image_list:\n",
    "    gt_path = dst_gt_image_path / ('_'.join(path.stem.split('_')[:-1]) + '.png')\n",
    "    if not gt_path.exists():\n",
    "        continue\n",
    "    gt_dst_path = dst_gt_image_path / (path.stem + '.png')\n",
    "    dst_path = dst_src_image_path / path.name\n",
    "    os.link(path, dst_path)\n",
    "    shutil.copy(gt_path, gt_dst_path)\n",
    "\n",
    "# ==========================================\n",
    "# 下面处理两个数据集的Summary文件\n",
    "\n",
    "dst_train_list = []\n",
    "dst_trainval_list = []\n",
    "dst_val_list = []\n",
    "\n",
    "# 对应的summary文件\n",
    "train_list = read_summary_file(train_summary_path)\n",
    "trainval_list = read_summary_file(trainval_summary_path)\n",
    "val_list = read_summary_file(val_summary_path)\n",
    "teeth_train_list = read_summary_file(teeth_train_summary_path)\n",
    "teeth_trainval_list = read_summary_file(teeth_trainval_summary_path)\n",
    "teeth_val_list = read_summary_file(teeth_val_summary_path)\n",
    "train_list.extend(teeth_train_list)\n",
    "trainval_list.extend(teeth_trainval_list)\n",
    "val_list.extend(teeth_val_list)\n",
    "\n",
    "for name in train_list:\n",
    "    files = list(map(lambda x: x.stem, dst_src_image_path.glob(name + '*')))\n",
    "    dst_train_list.extend(files)\n",
    "    \n",
    "for name in trainval_list:\n",
    "    files = list(map(lambda x: x.stem, dst_src_image_path.glob(name + '*')))\n",
    "    dst_trainval_list.extend(files)\n",
    "    \n",
    "for name in val_list:\n",
    "    files = list(map(lambda x: x.stem, dst_src_image_path.glob(name + '*')))\n",
    "    dst_val_list.extend(files)\n",
    "    \n",
    "write_summary_file(dst_train_summary_path, dst_train_list)\n",
    "write_summary_file(dst_trainval_summary_path, dst_trainval_list)\n",
    "write_summary_file(dst_val_summary_path, dst_val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对数据集的shape进行检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_base_path in ['aug_bright', 'aug_ori']:\n",
    "    dataset_base_path = base_path / dataset_base_path\n",
    "    src_image_path = dataset_base_path / 'JPEGImages'\n",
    "    gt_image_path = dataset_base_path / 'SegmentationClass'\n",
    "    \n",
    "    for path in src_image_path.glob('*.jpg'):\n",
    "        gt_path = gt_image_path / (path.stem + '.png')\n",
    "        \n",
    "        src_img = Image.open(path)\n",
    "        gt_img = Image.open(gt_path)\n",
    "        \n",
    "        if (src_img.size != (700, 700)):\n",
    "            src_img  = src_img.resize((700, 700))\n",
    "            src_img.save(path)\n",
    "            \n",
    "        if (gt_img.size != (700, 700)):\n",
    "            gt_img  = gt_img.resize((700, 700))\n",
    "            gt_img.save(gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
